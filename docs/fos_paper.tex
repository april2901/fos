\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{kotex}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{calc}
\usepackage{amsmath}
\usepackage{geometry}
\geometry{margin=1in}

% Title Information
\title{FoS (Focus on Speaking)\\
\large Group 12}
\author{
Sangyoon Kwon\\
\textit{Department of Computer Science}\\
\textit{Backend Development}\\
Seoul, Republic of Korea\\
\href{mailto:is0110@hanyang.ac.kr}{is0110@hanyang.ac.kr}
\and
Hyeyun Kwon\\
\textit{Department of Information Systems}\\
\textit{Frontend Development}\\
Seoul, Republic of Korea\\
\href{mailto:herakwon1124@hanyang.ac.kr}{herakwon1124@hanyang.ac.kr}
\and
Dohoon Kim\\
\textit{Department of Computer Science}\\
\textit{Backend Development}\\
Seoul, Republic of Korea\\
\href{mailto:april2901@hanyang.ac.kr}{april2901@hanyang.ac.kr}
\and
Seohyun Kim\\
\textit{Department of Information Systems}\\
\textit{Frontend Development}\\
Seoul, Republic of Korea\\
\href{mailto:dianwls0326@hanyang.ac.kr}{dianwls0326@hanyang.ac.kr}
\and
Daeun Lee\\
\textit{Division of Business Administration}\\
\textit{UI Design, PM}\\
Seoul, Republic of Korea\\
\href{mailto:shinran2929@hanyang.ac.kr}{shinran2929@hanyang.ac.kr}
\and
Minhyuk Jang\\
\textit{Division of Business Administration}\\
\textit{UI Design, PM}\\
Seoul, Republic of Korea\\
\href{mailto:jmh12230@hanyang.ac.kr}{jmh12230@hanyang.ac.kr}
}
\date{}

\begin{document}

\maketitle

\begin{center}
\url{https://github.com/april2901/ai-assistant-for-presentation}
\end{center}

\begin{abstract}
In modern business and educational environments,
meetings and presentations are key means of communication, and their
importance continues to grow. However, presenters and participants often
experience cognitive overload as they manage speech delivery, script
reference, slide transitions, and time management while also trying to
follow complex discussion flows and decisions. This leads to topic
drift, loss of focus, and unclear outcomes. To address these issues,
this project proposes an \textbf{LG display--linked real-time meeting AI
prompter} that extends a traditional teleprompter into an active,
context-aware assistant. The system listens to participants' speech,
interprets the meeting context in real time, and presents the next
required information---such as agenda structure, current topic,
decisions, and action items---on LG displays, while providing a private
coaching dashboard for the presenter or host. Core features include
real-time STT, flexible speech-to-script matching, keyword omission
detection, a real-time feedback dashboard, agenda visualization,
decision and action-item extraction, and fact-check widgets. A Meeting
Summary Report summarizes key topics, ideas, decisions, and action items
after the session. Through these functions, the project aims to improve
both individual presentation quality and overall meeting efficiency, and
to explore integration within LG's smart office ecosystem.
\end{abstract}

\noindent\textbf{Keywords:} Speech Recognition, Real-Time STT, Script
Synchronization, Real-Time Teleprompter, Slide Automation, Agenda
Tracking, Presentation Feedback, Human-Computer Interaction, Meeting
Intelligence

\section*{Role Assignment}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1188}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0953}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2644}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Roles}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Name}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Task description and etc.}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{User} & Daeun Lee, Minhyuk Jang & Tests the prototype from the
user's perspective, focusing on interface usability, speech
synchronization accuracy, and overall user experience. Provides
qualitative feedback for refinement. \\
\textbf{Customer} & LG Electronics (Assumed Client) & Defines
requirements for smart office presentation support software and
evaluates its feasibility for integration with LG's webOS-based business
ecosystem. \\
\textbf{Software Developer} & Sangyoon Kwon, Dohoon Kim (Backend),
Hyeyun Kwon, Seohyun Kim (Frontend) & Responsible for system
implementation including backend server logic, database management, API
communication, and frontend interface development. Ensures real-time
synchronization and stable slide automation. \\
\textbf{Development Manager\& UI Designer} & Daeun Lee, Minhyuk Jang &
Oversees project planning, documentation, and communication between
development teams. Manages task allocation, schedule tracking, design of
interface and quality assurance. \\
\end{longtable}
}

\section{Introduction}\label{introduction}

\textbf{A. Challenges in Modern Presentations and Meetings}

In professional and academic settings, presentations and meetings have
become essential tools for sharing ideas and making decisions. Yet
presenters and facilitators must simultaneously handle speech delivery,
script reference, slide transitions, and time tracking, while
participants struggle to follow complex discussion flows and remember
key points. This often causes interruptions in the presentation flow,
omission of important content, topic drift during discussions, and
unclear conclusions, ultimately reducing communication efficiency.

\textbf{B. Limitations of Existing Solutions}

Existing tools such as teleprompters, timers, and subtitle features
mainly provide static information or simple transcription. They are
useful for displaying text but do not actively intervene in real time to
prevent topic drift or support decision alignment. Many AI-based meeting
services focus on post-meeting summaries or minutes, which help review
what happened but do not improve the efficiency of the meeting while it
is in progress. As a result, core issues such as cognitive overload and
live meeting inefficiency remain unresolved.

\textbf{C. Project Goals and Proposed Solution}

This project proposes an integrated support system that covers
preparation, live delivery, and post-meeting feedback. The core concept
is an \textbf{``LG Display--Linked Real-Time Meeting AI Prompter.''} The
system continuously listens to meeting audio, analyzes the semantic
context of each utterance, and surfaces what the meeting needs next:
agenda structure, current topic, decisions, action items, and fact-check
results. At the same time, it provides a private teleprompter and
coaching dashboard for the presenter or host, helping with script
tracking, omission alerts, pacing, and gesture suggestions. The ultimate
goal is to reduce cognitive load and improve meeting focus and
decision-making speed.

\textbf{D. Dual-Screen Architecture for LG Displays}

To support both individual coaching and shared awareness, the system
adopts a dual-screen architecture. \textbf{Screen 1 (Presenter
Dashboard)} is shown on the presenter's personal device (e.g., LG Gram)
and provides a private teleprompter, omission alerts, pace metrics, and
AI suggestions. \textbf{Screen 2 (Shared Meeting Board)} is shown on LG
signage, LG One:Quick, or conference room TVs and visualizes slides, the
real-time agenda map, decisions and action items, and fact-check widgets
for all participants. This separation allows the presenter to receive
rich guidance without overwhelming the audience, while participants
share a clear view of where the meeting is and what has been decided.

\section{Requirements}\label{requirements}

\subsection{A. Before Presentation}

This phase focuses on the preparation process before a presenter begins
their presentation. Users interact with the system to upload materials,
create a script, and adjust content to fit the presentation environment.

\subsubsection{Slide-Script Alignment Recognition and Consulting}

When a user uploads a PPTX or PDF file, the system extracts textual and
visual elements using python-pptx and the Google Vision API (OCR). A
multimodal LLM processes these elements to interpret textual and
graphical contexts, generating a coherent draft script for each slide.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item OCR text recognition accuracy $\geq$ 95\%
\item Draft script grammatical accuracy $\geq$ 95\%
\item Slide--text coherence $\geq$ 95\%
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Presentation file (.pptx, .pdf)
\item \textbf{Output:} Structured text/image metadata, draft script (3--6 sentences per slide)
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Max 100 slides
\item Max file size 200 MB
\item Max 10 images per slide
\item Supported formats: PPTX, PDF
\end{itemize}

\subsubsection{Presentation Environment-Specific Script Adjustment}

The system adjusts the script's vocabulary level, tone, and length based
on the audience type (non-expert / practitioner / expert), target
presentation time, and speaker's pace. Using TensorFlow.js-based vision
models, audience facial expressions are analyzed every 3 seconds to
compute a ``focus score'' (0--100). When time is running short or
audience engagement decreases, an LLM provides real-time summaries or
interactive remarks.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Script adjustment time $\leq$ 5s
\item Focus detection accuracy $\geq$ 85\%
\item Timing deviation $\leq$ $\pm$5\%
\item Script fluency $\geq$ 90\%
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Audience type, target duration, speech rate(WPM), tone, audience video data
\item \textbf{Output:} Adjusted script (.txt), recommended timing table, real-time teleprompter feedback
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Camera $\geq$ 720p
\item Max 10 audience members detectable
\item LLM request frequency $\leq$ 1 per 10 s
\item Presentation $\leq$ 60 min
\end{itemize}

\subsection{B. Live Delivery Phase}

This phase involves real-time interaction between the user and the
system during the actual presentation. The system detects the
presenter's speech and performs instant support tasks
like synchronization, feedback, and suggestions.

\subsubsection{Real-time Teleprompter (Meeting Mode Support)}

The system transcribes the presenter's and participants' speech in real
time using Google Cloud or Naver Clova STT and aligns it with the
prepared script via KoSentence-BERT semantic similarity. The current
sentence is visually highlighted on the teleprompter. In meeting mode,
the STT pipeline can capture speech from multiple people in the room as
a single mixed audio stream; downstream modules operate on this combined
transcript.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Speech--script synchronization delay $\leq$ 1 s
\item Highlight accuracy $\geq$ 95\%
\item Alignment deviation $\leq$ 1 sentence
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Microphone audio (.wav, .mp3, $\geq$16 kHz), script file (.txt)
\item \textbf{Output:} Real-time highlighted script text and STT logs
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Session length $\leq$ 60 min
\item STT throughput $\geq$ 50 words/s
\item API cost $\approx$ \$0.006/min
\end{itemize}

\subsubsection{Automatic Slide Transition}

Through the Microsoft PowerPoint COM API, slides are automatically
advanced when the script reaches predefined transition points.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Transition latency $\leq$ 0.5 s
\item Transition accuracy $\geq$ 95\%
\item Failure rate $\leq$ 5\%
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Slide file (.pptx), predefined transition IDs
\item \textbf{Output:} Automatically advanced slide display
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Max 100 slides
\item Requires PowerPoint 2016 or later
\end{itemize}

\subsubsection{Flexible Speech-to-Script Matching}

The system maintains synchronization even when speech deviates lexically
from the script. Primary matching uses KoSentence-BERT vector similarity
(threshold $\geq$ 0.8), followed by secondary LLM-based contextual
verification if necessary.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Matching success rate $\geq$ 90\%
\item False match rate $\leq$ 5\%
\item Matching latency $\leq$ 0.3 s per sentence
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} STT transcript, script text
\item \textbf{Output:} Matched sentence ID and highlight position
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item LLM call limit $\leq$ 1 per second
\item STT buffering $\leq$ 5 s
\end{itemize}

\subsubsection{Key Content Omission Detection}

Detects missing predefined key phrases using cosine similarity (threshold $\geq$ 0.75) and alerts the presenter within 3 seconds.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Detection precision $\geq$ 95\%
\item False alarm $\leq$ 5\%
\item Alert delay $\leq$ 2 s
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} STT transcript, key phrase list ($\leq$50)
\item \textbf{Output:} Omission alert and log file
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Max 500 sentences compared
\item STT buffering interval: 5 s
\end{itemize}

\subsubsection{Real-time Script Reconstruction}

When omissions are detected, missing segments are asynchronously sent to
an LLM that generates supplementary sentences within 5 seconds. Approved
sentences are integrated into the script in real time.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Supplement generation $\leq$ 5 s
\item Contextual coherence $\geq$ 90\%
\item Integration success $\geq$ 90\%
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Missing sentence ID, context text, LLM API key
\item \textbf{Output:} Supplementary sentence, updated script
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Max 10 API calls per minute
\item Sentence length $\leq$ 100 characters
\end{itemize}

\subsubsection{Real-time Presenter Dashboard}

The system visualizes metrics such as words per minute (WPM), voice
volume, and progress rate using the Web Audio API, and applies
TensorFlow Lite models for basic emotion recognition (e.g.,
tension/calmness).

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Data refresh $\leq$ 2 s
\item Emotion inference error $\leq$ $\pm$5\%
\item Visualization accuracy $\geq$ 95\%
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Audio stream, STT logs
\item \textbf{Output:} Live dashboard showing progress, WPM, emotional state
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Sampling rate $\geq$ 16 kHz
\item Dashboard latency $\leq$ 1 s
\end{itemize}

\subsubsection{Speech Gesture Suggestions}

Before the presentation, the system analyzes slide images with a
multimodal LLM to detect key visual elements (graphs, photos, diagrams)
and map them to related keywords. During speech, when such keywords
appear in STT, gesture icons (e.g., pointing, emphasis) are displayed on
the teleprompter.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Suggestion latency $\leq$ 2 s
\item Gesture relevance $\geq$ 85\%
\item Recognition accuracy $\geq$ 90\%
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Slide images, script keywords
\item \textbf{Output:} Gesture icons displayed on teleprompter
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Max 3 visual mappings per keyword
\item Display duration: 2--3 s
\end{itemize}

\subsubsection{Real-Time Context and Intent Tagging}

The system analyzes STT text in real time and classifies each utterance
according to its intent and type. This information is used as the
foundation for the agenda map, decision board, and fact-check triggers.

\paragraph{Supported Tags}
\begin{itemize}
\item General comment
\item Idea proposal
\item Negative/Positive feedback
\item Decision
\item Request / Action Item
\item Question
\item Fact-check request
\end{itemize}

\paragraph{Acceptance Criteria}
\begin{itemize}
\item All utterances receive at least one tag from the predefined set
\item Intent classification accuracy $\geq$ 85\% on test samples
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} STT transcript segmented into utterances
\item \textbf{Output:} Tagged utterance stream (text + tag(s) + timestamp)
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Classification latency $\leq$ 0.5s per utterance
\item The tagging model must operate within the WebSocket round-trip time budget
\end{itemize}

\subsubsection{Real-Time Agenda Map}

Using the tagged utterance stream, the system builds a real-time agenda
map to prevent topic deviation and improve shared awareness. Each
emerging topic is registered as a node in a network graph displayed on
\textbf{Screen 2}.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Screen 2 displays a live STT log and its mapping to agenda nodes
\item Utterances tagged as ``Idea,'' ``Decision,'' or ``Action Item'' are grouped under appropriate agenda nodes
\item Nodes are color-coded by agenda type
\item The active topic is clearly highlighted
\item Node detail view shows STT snippet and timestamp
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Tagged utterance stream (from Requirement 8), semantic embeddings
\item \textbf{Output:} Real-time agenda network graph on Screen 2
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Graph update interval $\leq$ 2s
\item Max 30 agenda nodes per session
\end{itemize}

\subsubsection{Dual-Screen Synchronization between Presenter and Shared Display}

The system keeps Screen 1 (Presenter Dashboard) and Screen 2 synchronized
while respecting privacy boundaries.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Slide transition latency between Screen 1 and Screen 2 $\leq$ 0.5 s
\item No private elements (teleprompter text, omission alerts, AI suggestions) appear on Screen 2
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Slide control events, layout state, synchronization messages
\item \textbf{Output:} Consistent view of the current slide and agenda state on Screen 2
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item WebSocket synchronization interval $\leq$ 1s
\end{itemize}

\subsubsection{Real-time Decisions and Action Item Widget}

The system captures utterances tagged as decision or action item and
surfaces them in a dedicated widget on Screen 2, often placed below or
beside the agenda map.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Detection coverage for decision-like and action-like utterances $\geq$ 90\% on test scenarios
\item New items appear in the list within 2 seconds of the utterance
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Tagged utterance stream (Decision / Action Item tags)
\item \textbf{Output:} Real-time decision \& action-item list on Screen 2
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Max 100 items per session
\item Each item stored with its text content and timestamp, with a link to the original transcript segment
\end{itemize}

\subsubsection{Real-Time Fact-Check and Research Widget}

When the system detects a \textbf{Fact-check request} tag, it triggers a
lightweight research pipeline (RAG or web search) and surfaces the
result on Screen 2.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Successful keyword extraction for at least 90\% of fact-check requests
\item Research results displayed within 5 seconds
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Tagged utterance stream (Fact-check request tag), knowledge base or web search API
\item \textbf{Output:} Fact-check result widget with short answer and source link(s)
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Max 30 fact-check queries per session
\item Each query result limited to brief, conference-friendly summaries
\end{itemize}

\subsection{C. Post-Presentation Phase}

This phase involves the user receiving feedback on their presentation and conducting a Q\&A session after the presentation has concluded.

\subsubsection{Q\&A Auto-Response}

In Q\&A mode, the system uses Retrieval-Augmented Generation (RAG) to
search a pre-built database and generate 2--3 candidate answers, each
referencing supporting slides or pages.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Answer generation $\leq$ 5 s
\item Relevance score $\geq$ 0.85
\item Slide reference accuracy $\geq$ 98\%
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Question (speech/text), presentation DB (JSON)
\item \textbf{Output:} 2--3 candidate answers with referenced slides
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Max 20 questions per session
\item Max 300 tokens per answer
\item RAG cosine similarity $\geq$ 0.8
\end{itemize}

\subsubsection{Presentation and Meeting Analysis Report}

After the meeting or presentation, the system automatically analyzes collected data (speech logs, agenda map, decisions, action items, and referenced research results) and generates a Meeting Summary Report. This report covers time management, speech habits, content delivery, and meeting outcomes such as major topics, ideas, decisions, action items, and external facts referenced during the discussion.

\paragraph{Acceptance Criteria}
\begin{itemize}
\item Report generation $\leq$ 10 s
\item Analysis accuracy $\geq$ 95\%
\item User satisfaction $\geq$ 4.2/5.0
\end{itemize}

\paragraph{Input \& Output}
\begin{itemize}
\item \textbf{Input:} Speech logs, slide transitions, emotion data, agenda map, decision/action-item list, fact-check logs
\item \textbf{Output:} Analysis report (.html, .pdf)
\end{itemize}

\paragraph{Constraints}
\begin{itemize}
\item Max presentation time: 60 min
\item Max 100,000 words processed
\end{itemize}

\section{Version Control System}\label{version-control-system}

To manage source code and documentation, a version control system based
on \textbf{Git} was established.
A public repository named \textbf{``ai-assistant-for-presentation''} was
created on \textbf{GitHub}.

The following source code and documents have been uploaded to the
repository:

\begin{itemize}
\item All backend and frontend source code
\item Shared documents including this file (\textit{project\_documentation.md}) and other design files
\item Configuration files and project-related assets
\end{itemize}

All team members (development, project management, and UI/UX design)
have been granted access to the repository.
For efficient GitHub management, the team will adopt a \textbf{branch
management strategy}.

% Git branch diagram image - temporarily commented out
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.6\textwidth]{media/image1.png}
%\caption{Git Branch Management Strategy}
%\label{fig:git-branch}
%\end{figure}

\section{Development Environment}

\subsection{A. Choice of Software Development Platform}

\subsubsection{Platform Selection and Rationale}

The project adopts a web-based client--server architecture as its
primary development and deployment platform. The web environment ensures
platform-independent accessibility without requiring users to install
additional software, while providing seamless integration with
cloud-based APIs such as Google Cloud Speech-to-Text and
large-language-model (LLM) services. Modern web technologies, including
WebSockets, Web Audio API, and TensorFlow.js, enable the implementation
of essential real-time features such as live teleprompting and
synchronized feedback dashboards.

\subsubsection{Programming Languages and Rationale}

\begin{itemize}
\item \textbf{Backend:} Python (version 3.11 or higher) using FastAPI for asynchronous API and WebSocket communication. Python is the de facto standard for AI and machine learning workflows, offering a mature ecosystem that includes python-pptx, sentence-transformers, and google-cloud-speech libraries.
\item \textbf{Frontend:} TypeScript/JavaScript (Node.js 20 or higher) with React 18.2. JavaScript is the only natively supported browser language and is indispensable for client-side interaction. React combined with TypeScript supports modular, maintainable UI components and ensures type safety.
\end{itemize}

\subsubsection{Cost Estimation}

The estimated total development cost is approximately USD 30.00, as summarized below:

\begin{itemize}
\item Hardware: Personal laptops (MacBook Air/Pro) -- USD 0.00
\item Software and IDE: Visual Studio Code (free), Cursor Pro (USD 20 per month)
\item Cloud Services and APIs: AWS Free Tier, GCP STT and Vision API, OpenAI GPT Realtime Mini (approx. USD 10)
\end{itemize}

\subsubsection{Development Environment Details}

\begin{itemize}
\item Operating Systems: Windows 11, macOS 14 (Sonoma)
\item IDEs: Visual Studio Code (v1.90 or higher), Cursor (v1.7 or higher)
\item Version Control: Git (v2.39 or higher) and GitHub public repository (``ai-assistant-for-presentation'')
\item Backend Stack: Python 3.11+, FastAPI 0.110+, PostgreSQL 16 (Render hosted)
\item Frontend Stack: Node.js 20.10+, npm 10.2+, React 18.2+, TypeScript 5.2+
\item Major Libraries: python-pptx, sentence-transformers, websockets, TensorFlow.js, Web Audio API
\item Hardware Resources: Three personal laptops (two MacBook Airs, one MacBook Pro) used for development and testing.
\end{itemize}

\subsubsection{Use of Commercial Cloud Platforms}

\begin{itemize}
\item \textbf{Google Cloud Platform (GCP):} Utilized for Speech-to-Text and Vision OCR services to enable high-accuracy transcription and slide text extraction. Services operate within free-tier quotas.
\item \textbf{Amazon Web Services (AWS):} EC2 for backend deployment, S3 for file storage, RDS for database hosting, and Route 53 for domain management. Free-tier services are used for prototype deployment and demonstration.
\end{itemize}

\subsection{B. Software in Use}

Several existing software solutions and research studies were referenced
in designing the system:

\begin{itemize}
\item \textbf{PromptSmart (VoiceTrack):} A commercial teleprompter offering real-time voice tracking. The proposed system extends its capabilities by adding semantic matching, omission detection, and automated slide control.
\item \textbf{Microsoft PowerPoint (Live Subtitles):} Provides speech transcription but lacks contextual synchronization with scripts and automated slide transitions.
\end{itemize}

These benchmarks highlight the project's improvements in real-time
adaptivity and AI-driven presentation assistance.

\subsection{C. Task Distribution}

\begin{table}[h]
\centering
\begin{tabular}{|p{3cm}|p{3cm}|p{7cm}|}
\hline
\textbf{Role} & \textbf{Members} & \textbf{Responsibilities} \\
\hline
Backend Development & Sangyoon Kwon, Dohoon Kim & System architecture design, FastAPI server and WebSocket implementation, database schema (PostgreSQL), AI logic integration (STT, LLM, BERT), cloud deployment (AWS, Render) \\
\hline
Frontend Development & Hyeyun Kwon, Seohyun Kim & React-based UI implementation, client-side state management, real-time dashboard (Web Audio API), teleprompter interface, client-side AI (TensorFlow.js) \\
\hline
Project Management \& UI Design & Daeun Lee, Minhyuk Jang & Project planning and scheduling, UI/UX design (Figma), documentation and VCS management, user testing and feedback analysis \\
\hline
\end{tabular}
\caption{Team Task Distribution}
\label{tab:task-distribution}
\end{table}

\section{Specification}\label{specification}

\subsection{System Architecture Overview}

The overall system architecture is illustrated in Figure \ref{fig:architecture}. The diagram shows the flow of data from user input through the frontend (React), backend (API Routes), external services (Gemini), and database (Supabase/PostgreSQL).

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{architecture_diagram.png}
\caption{System Architecture Diagram}
\label{fig:architecture}
\end{figure}

The architecture consists of six main components:
\begin{enumerate}
\item User voice input captured through Web Speech API
\item Frontend (React) for user interface
\item Backend (API Routes) for business logic
\item External AI services (Gemini) for intelligent processing
\item Database management through Supabase
\item PostgreSQL database for persistent storage
\end{enumerate}

\subsection{Requirement 1: Real-Time Teleprompter}

This process involves a tightly coordinated real-time loop between the
client (web browser) and the server (Python backend) through WebSocket
communication.

\subsubsection{Client Initialization}

When the user presses ``Start Presentation,'' the React frontend
requests microphone access using \texttt{navigator.mediaDevices.getUserMedia()}
and establishes a secure WebSocket (\texttt{wss://}) connection to the backend
API server.
The Web Audio API initializes an \texttt{AudioContext} and \texttt{ScriptProcessorNode}
to capture raw audio chunks.

\subsubsection{Client-Side Real-Time Audio Streaming}

The \texttt{ScriptProcessorNode} continuously triggers \texttt{onaudioprocess} events
(e.g., every 500 ms).
Each raw audio buffer (16-bit PCM) is sent to the backend server
through WebSocket.

\subsubsection{Server-Side STT and Synchronization}

The backend receives the audio chunks and streams them to the Google
Cloud Speech-to-Text API.
The API returns \textit{interim} (fast but less accurate) and
\textit{final} (slower but more accurate) transcripts.
When a final sentence is received, it is appended to the full
transcript of the session.
The backend then calls the \texttt{FlexibleSpeechMatcher} service to locate the
new \texttt{currentSentenceIndex} within the user's script.

\subsubsection{Server Broadcast}

The server immediately sends a WebSocket message to the client:\\
\texttt{\{ "action": "UPDATE\_TELEPROMPTER", "index": currentSentenceIndex \}}

\subsubsection{Client Update}

The frontend WebSocket listener receives the message and updates the
highlighted text accordingly, scrolling the teleprompter to the
current index in real time.

\subsection{Requirement 2: Flexible Speech-to-Script Matching}

This algorithm implements a hybrid matching mechanism combining fast
vector similarity and fallback large-language-model (LLM) validation.

\subsubsection{Pseudocode Overview}

\begin{verbatim}
SIMILARITY_THRESHOLD = 0.75
SEARCH_WINDOW = 5
LLM_VALIDATION_THRESHOLD = 0.60

Function FindCurrentPosition(fullTranscript, scriptSentences, lastIndex):
    latestTranscript = GetLastNWords(fullTranscript, 10)
    transcriptVector = KoSentenceBERT.encode(latestTranscript)
    
    searchStart = lastIndex
    searchEnd = min(lastIndex + SEARCH_WINDOW, len(scriptSentences))
    searchWindow = scriptSentences[searchStart : searchEnd]
    
    bestMatchIndex = -1
    highestSimilarity = 0
    
    # Step 1: Fast vector similarity
    for index, sentence in enumerate(searchWindow):
        similarity = CosineSimilarity(transcriptVector, sentence.vector)
        if similarity > highestSimilarity:
            highestSimilarity = similarity
            bestMatchIndex = searchStart + index
    
    # Step 2: Omission check
    if bestMatchIndex > lastIndex:
        CheckForOmissions(lastIndex, bestMatchIndex, scriptSentences)
    
    if highestSimilarity >= SIMILARITY_THRESHOLD:
        return bestMatchIndex
    
    # Step 3: LLM fallback validation
    if highestSimilarity >= LLM_VALIDATION_THRESHOLD:
        prompt = CreateLLMPrompt(latestTranscript, searchWindow)
        AsyncCallLLM(prompt, HandleLLMResult)
        return bestMatchIndex
    
    return lastIndex
\end{verbatim}

This process ensures low latency while maintaining semantic accuracy
through adaptive matching.

\subsection{Requirement 3: Key Content Omission Detection}

This function integrates directly with the flexible matching process.
When a skipped section is detected, the system checks whether any
omitted sentence contains a predefined \textit{key phrase} and alerts the
presenter.

\subsubsection{Database Preparation}

When users upload a script, each sentence is marked with a boolean
attribute \texttt{isKeyPhrase} (true / false) and stored in the database.

\subsubsection{Server-Side Omission Detection}

When \texttt{FindCurrentPosition()} identifies a new \texttt{bestMatchIndex} greater
than \texttt{lastIndex + 1}, the server calls \texttt{CheckForOmissions()}.

\subsubsection{Omission Logic}

If skipped sentences are found between the two indices, each is
inspected for \texttt{isKeyPhrase == true}.
When detected, the omitted sentence index is flagged, and the
following message is broadcast:\\
\texttt{\{ "action": "OMISSION\_DETECTED", "index": omittedSentenceIndex \}}

\subsubsection{Client Notification}

The frontend highlights the corresponding part of the script (e.g.,
flashing red or adding a border) to visually warn the presenter in
real time.
Simultaneously, an asynchronous script-reconstruction task
(Requirement 4) is triggered.

\subsection{Requirement 4: Real-Time Script Reconstruction}

This asynchronous process generates short ``bridging sentences''
whenever key content omissions are detected, ensuring smooth narrative
flow without latency in the main loop.

\subsubsection{Asynchronous Trigger}

\texttt{CheckForOmissions()} launches \texttt{HandleOmissionAsynchronously()} in a
separate asynchronous task.

\subsubsection{Prompt Generation for LLM}

The function combines three inputs:
\begin{enumerate}
\item the omitted sentence
\item the current context sentences
\item an instruction prompt such as:
\end{enumerate}

\textit{``You are a presentation coach. The presenter accidentally omitted `[omittedSentence]' and is now moving to `[contextSentences]'. Please generate one short, natural bridging sentence in Korean that connects these topics smoothly.''}

\subsubsection{LLM API Invocation}

The backend asynchronously requests an LLM (e.g., GPT or Gemini) to
generate the bridging sentence.

\subsubsection{Response Delivery}

Upon success, the server sends the message:\\
\texttt{\{ "action": "SCRIPT\_SUGGESTION", "text": llm\_generated\_sentence \}}

\subsubsection{Client Interface Behavior}

The React frontend displays the generated sentence in the AI
Suggestion section as an alert message:
``Do you approve this suggestion?'' with \textbf{Accept (\#0064FF)} and \textbf{No (\#E0E6EA)} buttons.
If the user selects \textit{Accept}, the updated script is applied, and
a small alert ``Update complete'' appears at the top-right corner.

\end{document}